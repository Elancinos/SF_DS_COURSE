{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK предлагает удобные инструменты для множества задач NLP: токенизация, стемминг, лемматизация, морфологический и синтаксический анализ, а также анализ настроений. В NLTK включены корпуса текстов и словарные ресурсы, такие как WordNet, позволяющие работать с огромным объемом текстовых данных. Это делает NLTK мощным инструментом для анализа и обработки текста на разных языках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <a id =\"table_of_content\"> Оглавление</a>\n",
    "\n",
    "[Параметры](#main_params)\n",
    "\n",
    "[Токенизация](#tokenization)\n",
    "\n",
    "[Удаление стоп-слов](#stop-words)\n",
    "\n",
    "[Стемминг](#stemming)\n",
    "\n",
    "[Лемматизация](#lemmatization)\n",
    "\n",
    "[Анализ настроения](#sentiment_analysis)\n",
    "\n",
    "[Bag Of Words](#bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <a id='main_params'> Параметры NLTK</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка популярных слов и словосочетаний\n",
    "\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[К оглавлению](#table_of_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <a id='tokenization'> Токенизация </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация — это процесс разбиения текста на более мелкие части, такие как слова или предложения. Это первый шаг в анализе текста, который позволяет преобразовать непрерывный текст в дискретные элементы, с которыми можно работать отдельно. Этот процесс помогает в выявлении ключевых слов и фраз, а также в упрощении последующего анализа текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разбиение на слова\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'упрощает', 'обработку', 'текста', '.']\n"
     ]
    }
   ],
   "source": [
    "# Токенизация текста\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLTK упрощает обработку текста.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'My', 'name', 'is', 'Aman', 'Kharwal', ',', 'I', 'am', 'here', 'to', 'guide', 'you', 'to', 'your', 'journey', 'in', 'Machine', 'Learning', 'for', 'free', '.']\n",
      "['Hi', ',', 'My', 'name', 'Aman', 'Kharwal', ',', 'I', 'guide', 'journey', 'Machine', 'Learning', 'free', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Hi, My name is Aman Kharwal, I am here to guide you to your journey in Machine Learning for free.\"\n",
    "tokens = word_tokenize(text)\n",
    "tokenization = [word for word in tokens if not word in stopwords.words('english')]\n",
    "print(tokens)\n",
    "print(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OTUS.', 'Наш сайт https://otus.ru/.']\n"
     ]
    }
   ],
   "source": [
    "# Разбиение на предложения\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"OTUS. Наш сайт https://otus.ru/.\"\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация полезна в задачах, где необходимо анализировать отдельные слова или фразы, например, при определении ключевых слов в тексте, анализе частотности слов или при обучении моделей машинного обучения для классификации текста.\n",
    "\n",
    "[К оглавлению](#table_of_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <a id='stop-words'> Удаление стоп-слов</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп-слова — это общеупотребительные слова в языке, которые обычно несут мало смысловой нагрузки (например, \"и\", \"в\", \"на\"). Их удаление позволяет сократить объем данных для анализа и сосредоточиться на более значимых словах, что повышает точность и эффективность обработки текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка модуля\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'помогает', 'удалении', 'стоп-слов', 'текста', '.']\n"
     ]
    }
   ],
   "source": [
    "#Удаление стоп-слов на русском\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLTK помогает в удалении стоп-слов из текста.\"\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'helps', 'removing', 'stopwords', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "#Удаление стоп-слов на английском\n",
    "\n",
    "text = \"NLTK helps in removing stopwords from the text.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word for word in tokens if not word in stopwords.words('english')]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление стоп-слов часто юзается в задачах обработки текста, таких как анализ настроений, классификация текстов, создание облаков слов и в информационном поиске, где важно выделить ключевую информацию из текста.\n",
    "\n",
    "[К оглавлению](#table_of_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <a id='stemming'> Стемминг</a>\n",
    "\n",
    "P.S. Стемминг — это процесс сведения слов к их основной (корневой) форме, удаляя окончания и суффиксы. Это помогает уменьшить сложность текста и улучшить производительность алгоритмов анализа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'stem', 'form', 'of', 'leav', 'is', 'leaf']\n",
      "['листовые', 'листочки', 'лист', 'листва', 'листве', 'почему', 'так']\n"
     ]
    }
   ],
   "source": [
    "# Стемминг на английском\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "text = \"The stemmed form of leaves is leaf\"\n",
    "text_rus = \"Листовые листочки лист листва листве почему так\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens_rus = word_tokenize(text_rus)\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "stemmed_words_rus = [stemmer.stem(word) for word in tokens_rus]\n",
    "print(stemmed_words)\n",
    "print(stemmed_words_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'stemmed', 'form', 'of', 'leaves', 'is', 'leaf']\n",
      "['листов', 'листочк', 'лист', 'листв', 'листв', 'поч', 'так']\n"
     ]
    }
   ],
   "source": [
    "# Стемминг на русском\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_rus = SnowballStemmer(\"russian\")\n",
    "text = \"The stemmed form of leaves is leaf\"\n",
    "text_rus = \"Листовые листочки лист листва листве почему так\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens_rus = word_tokenize(text_rus)\n",
    "stemmed_words = [stemmer_rus.stem(word) for word in tokens]\n",
    "stemmed_words_rus = [stemmer_rus.stem(word) for word in tokens_rus]\n",
    "print(stemmed_words)\n",
    "print(stemmed_words_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг наиболее полезен в задачах, где важно уменьшить разнообразие словоформ, например, при индексации текста для поисковых систем, в аналитике текстов большого объема и при обучении моделей машинного обучения для классификации или кластеризации текстов.\n",
    "\n",
    "[К оглавлению](#table_of_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <a id='lemmatization'> Лемматизация </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от стемминга, лемматизация сводит слова к их лемме — это более сложный процесс, который учитывает морфологический анализ слов. Лемматизация более точно обрабатывает слова, приводя их к словарной форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Загрузка доп. модуля\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'lemmatized', 'form', 'of', 'leaf', 'is', 'leaf']\n"
     ]
    }
   ],
   "source": [
    "#Лемматизация на английском языке\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"The lemmatized form of leaves is leaf\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лемматизирова', 'форм', 'слов', 'лист', 'эт', 'лист']\n"
     ]
    }
   ],
   "source": [
    "#Лемматизация на английском языке возможна только через стеммер\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "text = \"Лемматизированная форма слова листья это лист\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация важна в задачах, где требуется высокая точность обработки текста, таких как машинный перевод, семантический анализ текста и создание систем вопросов-ответов, где важно точно понимать значение слов в контексте.\n",
    "\n",
    "[К оглавлению](#table_of_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <a id='sentiment_analysis'> Анализ настроения </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ настроений, иногда называемый \"определением тональности\", включает использование NLP, статистических или машинно-обученных алгоритмов для изучения, идентификации и извлечения информации о настроениях из текстов. Он может быть столь же простым, как определение положительной или отрицательной окраски отзыва, или настолько сложным, как определение более тонких эмоциональных состояний, таких как ирония или разочарование.\n",
    "\n",
    "Анализ настроений не без проблем. Одна из основных сложностей заключается в интерпретации сарказма, иронии и фигуративного языка. Например, фраза \"Ну да, конечно, мне очень понравилось, когда мой телефон перестал работать\" на самом деле выражает разочарование, хотя на первый взгляд может показаться положительной. Распознавание таких тонкостей требует продвинутых алгоритмов и, часто, контекстуального анализа.\n",
    "\n",
    "Анализ настроений (или сентимент-анализ) в NLTK часто сводится к классификации текста на позитивный или негативный. Чтобы реализовать анализ настроений, можно использовать разные подходы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.431, 'pos': 0.569, 'compound': 0.7644}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Простая классификация с использованием \n",
    "#предварительно обученных данных\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "text = \"NLTK is amazing for natural language processing!\"\n",
    "print(sia.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'neg_tagged_word_feats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m testing_docs \u001b[38;5;241m=\u001b[39m test_subj_docs\u001b[38;5;241m+\u001b[39mtest_obj_docs\n\u001b[0;32m     22\u001b[0m sentim_analyzer \u001b[38;5;241m=\u001b[39m SentimentAnalyzer()\n\u001b[1;32m---> 23\u001b[0m all_words_neg \u001b[38;5;241m=\u001b[39m \u001b[43mneg_tagged_word_feats\u001b[49m(sentim_analyzer\u001b[38;5;241m.\u001b[39mall_words(training_docs), mark_negation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m unigram_feats \u001b[38;5;241m=\u001b[39m sentim_analyzer\u001b[38;5;241m.\u001b[39munigram_word_feats(all_words_neg, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     25\u001b[0m sentim_analyzer\u001b[38;5;241m.\u001b[39madd_feat_extractor(extract_unigram_feats, unigrams\u001b[38;5;241m=\u001b[39munigram_feats)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'neg_tagged_word_feats' is not defined"
     ]
    }
   ],
   "source": [
    "#Классификация с использованием настраиваемых \n",
    "# тренировочных данных\n",
    "\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "nltk.download('subjectivity')\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = neg_tagged_word_feats(sentim_analyzer.all_words(training_docs), mark_negation=True)\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.75, subjectivity=0.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Анализ настроения с помощью textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "text = \"I love NLTK. It's incredibly helpful!\"\n",
    "blob = TextBlob(text)\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Анализ настроений с помощью токенизатора и списка стоп-слов\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"NLTK is not bad for learning NLP.\"\n",
    "filtered_text = ' '.join([word for word in word_tokenize(text) if not word in stop_words])\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "print(sia.polarity_scores(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.453, 'neu': 0.547, 'pos': 0.0, 'compound': -0.7002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Комбинирование лемматизации и анализа настроений\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"The movie was not good. The plot was terrible!\"\n",
    "lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "print(sia.polarity_scores(lemmatized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[К оглавлению](#table_of_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <a id='bag_of_words'> Bag Of Words </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель \"Мешок слов\" (Bag of Words, BoW) является основным методом представления текстовых данных в обработке естественного языка (Natural Language Processing, NLP). Она преобразует текст в числовой вектор, где каждое слово в тексте представляется количеством его появлений.\n",
    "\n",
    "Модель BoW\n",
    "В модели BoW текст (например, предложение или документ) представляется в виде \"мешка\" его слов, не учитывая грамматику и порядок слов, но сохраняя мультиплицивность. Это преобразование текста в набор чисел позволяет использовать стандартные методы машинного обучения, которые работают на числовых данных.\n",
    "\n",
    "Каждое уникальное слово в тексте соответствует определенному индексу (или \"слоту\") в векторе. Если слово встречается в тексте, то в соответствующем слоте вектора записывается количество его появлений. Например, текст \"яблоко банан яблоко\" превратится в вектор [2, 1], если индекс 0 соответствует слову \"яблоко\", а индекс 1 — слову \"банан\".\n",
    "\n",
    "В анализе настроений модель BoW используется для преобразования текстовых данных в формат, пригодный для алгоритмов машинного обучения. Так, текстовые данные (например, отзывы пользователей) преобразуются в числовые векторы, на которых можно обучать классификаторы для определения, например, позитивного или негативного отношения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Создание BoW с NLTK и его использование для классификации\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Пример данных\n",
    "texts = [\"I love this product\", \"This is a bad product\", \"I dislike this\", \"This is the best!\"]\n",
    "labels = [1, 0, 0, 1]  # 1 - позитивный, 0 - негативный\n",
    "\n",
    "# Токенизация\n",
    "tokens = [word_tokenize(text) for text in texts]\n",
    "\n",
    "# Создание BoW модели\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform([' '.join(token) for token in tokens])\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size=0.3)\n",
    "\n",
    "# Обучение классификатора\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\telis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8166666666666667\n"
     ]
    }
   ],
   "source": [
    "#Пример 2. Простой анализ настроений с BoW\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Загрузка данных\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Подготовка данных\n",
    "texts = [' '.join(doc) for doc, _ in documents]\n",
    "labels = [1 if category == 'pos' else 0 for _, category in documents]\n",
    "\n",
    "# Создание BoW модели\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, labels, test_size=0.3)\n",
    "\n",
    "# Обучение классификатора\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Оценка классификатора\n",
    "predictions = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[К оглавлению](#table_of_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
