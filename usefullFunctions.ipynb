{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Полезные функции\n",
    "\n",
    "<center> Сюда буду добавлять полезные функции, которые могут пригодиться в будущем. В комментарии может быть код примера работы или дополнительные фичи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# функция, которая принимает количество кластеров для k-means и матрицу с признаками объектов и возвращает инерцию \n",
    "def get_inertia(cluster_num, X):\n",
    "# инициализируем алгоритм кластеризации\n",
    "    k_means =  KMeans(n_clusters=cluster_num, random_state=42)\n",
    "# запускаем алгоритм k-means\n",
    "    k_means.fit(X)\n",
    "# находим значение инерции\n",
    "    inertia = k_means.inertia_\n",
    "# возвращаем значение инерции\n",
    "    return inertia\n",
    "\n",
    "# # создаём пустой список для значений инерции\n",
    "# inertia = []\n",
    "# # итерируемся по разным размерам кластеров (от 1 до 9) и сохраняем значение инерции для каждого кластера\n",
    "# for cluster_num in range(1, 10):\n",
    "# # сохраняем значения\n",
    "#     inertia.append(get_inertia(cluster_num, X))\n",
    "\n",
    "# # визуализируем, как менялась инерция в зависимости от количества кластеров\n",
    "# # задаём названия осям x и y\n",
    "# plt.xlabel(\"cluster\", fontsize=12)\n",
    "# plt.ylabel(\"inertia\", fontsize=12)\n",
    "# # рисуем изменение инерции\n",
    "# plt.plot([i for i in range(1, 10)], inertia, 'xb-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем метрику силуэта\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# напишем функцию, как и при подсчете метода локтя\n",
    "def get_silhouette(cluster_num, X):\n",
    "    k_means = KMeans(n_clusters=cluster_num, init='k-means++', n_init=10, random_state=42)\n",
    "    k_means.fit(X)\n",
    "# подсчитаем метрику силуэта, передав данные и то, к каким кластерам относятся объекты\n",
    "    silhouette = silhouette_score(X, k_means.predict(X))\n",
    "    return silhouette\n",
    "\n",
    "# # создадим пустой словарь, ключами будут инерция и количество кластеров\n",
    "# silhouette_res = {\"silhouette\": [], \"cluster\": []}\n",
    "\n",
    "# # выберем нужные данные \n",
    "# X = df[['Attack', 'Defense']]\n",
    "\n",
    "# for cluster_num in range(2, 10):\n",
    "#     silhouette_res[\"silhouette\"].append(get_silhouette(cluster_num, X))\n",
    "#     silhouette_res[\"cluster\"].append(cluster_num)\n",
    "    \n",
    "# # сохраним в датафрейм значение силуэта и количество кластеров\n",
    "# silhouette_df = pd.DataFrame(silhouette_res)\n",
    "\n",
    "# # установим стиль для визуализиции\n",
    "# sns.set_style(\"darkgrid\")\n",
    "# # визуализируем зависимость значения инерции от количества кластеров\n",
    "# sns.lineplot(data=silhouette_df, x=\"cluster\", y=\"silhouette\", marker= \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "\n",
    "# напишем функцию, которая будет проводить алгомеративную кластеризацию\n",
    "# на вход она будет принимать X — матрицу с признаками для кластеризации и n_clusters — количество кластеров,\n",
    "# на выходе будет возвращать список с номерами кластеров\n",
    "def get_aggl_clustering(X, n_clusters):\n",
    "    # запустим агломеративную кластеризацию\n",
    "    agglomerative_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    # обучим модель агломеративной кластеризации\n",
    "    agglomerative_clustering.fit(X)\n",
    "    # получим список c информацией, к какому кластеру относятся объекты\n",
    "    aggl_prediction = agglomerative_clustering.labels_\n",
    "    # вернём список с результатами\n",
    "    return aggl_prediction\n",
    "\n",
    "# напишем функцию для подсчёта коэффициента силуэта\n",
    "def get_silhouette_agg_clust(X, cluster_num):\n",
    "    # проведём агломеративную кластеризацию с заданным количеством кластеров\n",
    "    res = get_aggl_clustering(X, cluster_num)\n",
    "    # рассчитаем коэффициент силуэта\n",
    "    silhouette = silhouette_score(X, res)\n",
    "    return silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метод межквартильного размаха (метод Тьюки)\n",
    "#Модифицированная функция с настраиваемыми IQR, и проверкой на логарифмирование\n",
    "#Подробнее в PY-14\n",
    "import numpy as np\n",
    "\n",
    "def outliers_iqr_mod(data, feature, log_scale=False, left=1.5, right=1.5):\n",
    "    \n",
    "    \"\"\"Функция для очистки дата-фрейма от выбросов по методу межквартильного размаха (метода Тьюки).\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): дата-фрейм с данными.\n",
    "        feature (pd.Series): признак (столбец) из дата-фрейма. \n",
    "        log_scale (bool, optional): Логарифмирование в процессе работы функции. Defaults to False.\n",
    "        left (float, optional): левый интервал, за пределами которого значения считаются выбросами. Defaults to 1.5.\n",
    "        right (float, optional): правый интервал, за пределами которого значения считаются выбросами. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        outliners (DataFrame): дата-фрейм со строками, которые функция посчитала выбросами.\n",
    "        cleaned (DataFtame): очищенный от выбросов дата-фрейм. \n",
    "    \"\"\"    \n",
    "    \n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75)\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * left)\n",
    "    upper_bound = quartile_3 + (iqr * right)\n",
    "    outliners = data[(x < lower_bound)|(x > upper_bound)]\n",
    "    cleaned = data[(x >= lower_bound)&(x <= upper_bound)]\n",
    "    return outliners, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#метод z-отклонений (метод сигм)\n",
    "import numpy as np\n",
    "\n",
    "def outliers_z_score_mod(data, feature, log_scale=False,\n",
    "                         left=3, right=3):\n",
    "    \n",
    "    \"\"\"Функция для очистки дата-фрейма от выбросов по методу z-отклонений (методу сигм)\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): дата-фрейм с данными.\n",
    "        feature (pd.Series): признак (столбец) из дата-фрейма. \n",
    "        log_scale (bool, optional): Логарифмирование в процессе работы функции. Defaults to False.\n",
    "        left (float, optional): левый интервал, за пределами которого значения считаются выбросами. Defaults to 3.\n",
    "        right (float, optional): правый интервал, за пределами которого значения считаются выбросами. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        outliners (DataFrame): дата-фрейм со строками, которые функция посчитала выбросами.\n",
    "        cleaned (DataFtame): очищенный от выбросов дата-фрейм. \n",
    "    \"\"\"    \n",
    "    \n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    mu = x.mean()\n",
    "    sigma = x.std()\n",
    "    lower_bound = mu - left*sigma\n",
    "    upper_bound = mu + right*sigma\n",
    "    outliers = data[(x<lower_bound) | (x>upper_bound)]\n",
    "    cleaned = data[(x>lower_bound) & (x<upper_bound)]\n",
    "    return outliers, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_corr_heatmap(data, columns_list, title, fontsize=18, method='pearson'):\n",
    "    \n",
    "    \"\"\"Функция для построения тепловой карты корреляций столбцов дата-фрейма.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): дата-фрейм, по которому нужно построить тепловую карту.\n",
    "        columns_list (list): список столбцов, по которым нужно построить тепловую карту.\n",
    "        title (str): будущий заголовок тепловой карты.\n",
    "        fontsize (int): размер шрифта для заголовка. По умолчанию равен 18.\n",
    "        method (str, optional): метод корреляции признаков. По умолчанию используется корреляция Пирсона.\n",
    "    \"\"\"    \n",
    "    \n",
    "    fig_, ax_ = plt.subplots(figsize=(15, 15))\n",
    "    corr = data[columns_list].corr(method)\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    sns.heatmap(corr,\n",
    "                annot=True,\n",
    "                linewidths=0.1,\n",
    "                ax=ax_,\n",
    "                mask=mask,\n",
    "                cmap='viridis',\n",
    "                fmt='.1g')\n",
    "    ax_.set_title(title, fontsize=fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "\n",
    "def plot_learning_curve(model, X, y, cv, scoring=\"f1\", ax=None, title=\"\"):\n",
    "    \n",
    "    \"\"\"Функция для построения графика кривых обучения.\n",
    "\n",
    "    Args:\n",
    "        model (sklearn): Модель, для которой строится график.\n",
    "        X (pd.DataFrame): Набор параметров на вход для модели.\n",
    "        y (pd.Series): Значения целевого признака обучения.\n",
    "        cv (sklearn.model_selection): Кросс-валидатор\n",
    "        scoring (str, optional): Метрика оценивания модели. Defaults to \"f1\".\n",
    "        ax (_type_, optional): Координатная плоскость matplotlib. Defaults to None.\n",
    "        title (str, optional): Заголовок графика. Defaults to \"\".\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Вычисляем координаты для построения кривой обучения\n",
    "    train_sizes, train_scores, valid_scores = model_selection.learning_curve(\n",
    "        estimator=model,  # модель\n",
    "        X=X,  # матрица наблюдений X\n",
    "        y=y,  # вектор ответов y\n",
    "        cv=cv,  # кросс-валидатор\n",
    "        scoring=scoring,  # метрика\n",
    "    )\n",
    "    # Вычисляем среднее значение по фолдам для каждого набора данных\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    # Если координатной плоскости не было передано, создаём новую\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))  # фигура + координатная плоскость\n",
    "    # Строим кривую обучения по метрикам на тренировочных фолдах\n",
    "    ax.plot(train_sizes, train_scores_mean, label=\"Train\")\n",
    "    # Строим кривую обучения по метрикам на валидационных фолдах\n",
    "    ax.plot(train_sizes, valid_scores_mean, label=\"Valid\")\n",
    "    # Даём название графику и подписи осям\n",
    "    ax.set_title(\"Learning curve: {}\".format(title))\n",
    "    ax.set_xlabel(\"Train data size\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    # Устанавливаем отметки по оси абсцисс\n",
    "    ax.xaxis.set_ticks(train_sizes)\n",
    "    # Устанавливаем диапазон оси ординат\n",
    "    ax.set_ylim(0, 1)\n",
    "    # Отображаем легенду\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для оптимизации с hyperopt\n",
    "def hyperopt_rf(params, cv=5, X=X_train_scaled, y=y_train, random_state=random_state):\n",
    "    # функция получает комбинацию гиперпараметров в \"params\"\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "             'min_samples_leaf': int(params['min_samples_leaf'])\n",
    "              }\n",
    "  \n",
    "    # используем эту комбинацию для построения модели\n",
    "    model = ensemble.RandomForestClassifier(**params, random_state=random_state)\n",
    "\n",
    "    # обучаем модель\n",
    "    model.fit(X, y)\n",
    "    score = metrics.f1_score(y, model.predict(X))\n",
    "    \n",
    "    # обучать модель можно также с помощью кросс-валидации\n",
    "    # применим  cross validation с тем же количеством фолдов\n",
    "    # score = cross_val_score(model, X, y, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "\n",
    "    # метрику необходимо минимизировать, поэтому ставим знак минус\n",
    "    return -score"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
